"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ OCR –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python evaluate_dataset.py --model model.pth --charset charset.txt --csv labels.csv --root images/
"""

import os
import argparse
import pandas as pd
from tqdm import tqdm
import numpy as np

from inference import OCRInference
from training.metrics import character_error_rate, word_error_rate, compute_accuracy


def load_dataset(csv_path, root_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ CSV —Ñ–∞–π–ª–∞."""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
    
    if not os.path.exists(root_path):
        raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {root_path}")
    
    # –ß–∏—Ç–∞–µ–º CSV
    df = pd.read_csv(csv_path)
    
    # –û–∂–∏–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏: filename, text
    if 'filename' not in df.columns or 'text' not in df.columns:
        raise ValueError("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'filename' –∏ 'text'")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–µ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
    image_paths = []
    texts = []
    
    for idx, row in df.iterrows():
        filename = row['filename']
        text = str(row['text'])
        
        # –ò—â–µ–º —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ
        image_path = os.path.join(root_path, filename)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ
        if not os.path.exists(image_path):
            for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:
                test_path = os.path.join(root_path, filename + ext)
                if os.path.exists(test_path):
                    image_path = test_path
                    break
        
        if os.path.exists(image_path):
            image_paths.append(image_path)
            texts.append(text)
        else:
            print(f"‚ö†Ô∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}")
    
    return image_paths, texts


def evaluate_model(model_path, charset_path, csv_path, root_path, batch_size=16, max_samples=None, img_h=32, img_w=128):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ."""
    
    print(f"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ")
    print(f"üìÇ –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"üìÇ Charset: {charset_path}")
    print(f"üìÇ CSV: {csv_path}")
    print(f"üìÇ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {root_path}")
    print(f"üìê –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_h}x{img_w}")
    print("-" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    ocr = OCRInference(model_path, charset_path, device="auto", img_h=img_h, img_w=img_w)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print("üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...")
    image_paths, true_texts = load_dataset(csv_path, root_path)
    
    if max_samples:
        image_paths = image_paths[:max_samples]
        true_texts = true_texts[:max_samples]
    
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(image_paths)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    if len(image_paths) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏!")
        return
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("üöÄ –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    predicted_texts = []
    
    # –ë–∞—Ç—á–µ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    for i in tqdm(range(0, len(image_paths), batch_size), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞"):
        batch_paths = image_paths[i:i + batch_size]
        batch_predictions = ocr.predict(batch_paths, batch_size=batch_size)
        predicted_texts.extend(batch_predictions)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏...")
    
    # –¢–æ—á–Ω–æ—Å—Ç—å (exact match)
    accuracy = compute_accuracy(true_texts, predicted_texts)
    
    # CER (Character Error Rate)
    cers = [character_error_rate(true, pred) for true, pred in zip(true_texts, predicted_texts)]
    avg_cer = np.mean(cers)
    
    # WER (Word Error Rate) 
    wers = []
    for true, pred in zip(true_texts, predicted_texts):
        try:
            wer = word_error_rate(true, pred)
            wers.append(wer)
        except:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏) —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ 100% –æ—à–∏–±–∫—É
            wers.append(1.0)
    avg_wer = np.mean(wers)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*60)
    print("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
    print("="*60)
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(image_paths)}")
    print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å (Exact Match): {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"üî§ –°—Ä–µ–¥–Ω–∏–π CER: {avg_cer:.4f} ({avg_cer*100:.2f}%)")
    print(f"üìù –°—Ä–µ–¥–Ω–∏–π WER: {avg_wer:.4f} ({avg_wer*100:.2f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
    print(f"\nüìâ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫:")
    print(f"CER: –º–∏–Ω={min(cers):.3f}, –º–∞–∫—Å={max(cers):.3f}, –º–µ–¥–∏–∞–Ω–∞={np.median(cers):.3f}")
    print(f"WER: –º–∏–Ω={min(wers):.3f}, –º–∞–∫—Å={max(wers):.3f}, –º–µ–¥–∏–∞–Ω–∞={np.median(wers):.3f}")
    
    # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
    print(f"\n‚ùå –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫ (—Ç–æ–ø-5 –ø–æ CER):")
    error_data = list(zip(true_texts, predicted_texts, cers))
    error_data.sort(key=lambda x: x[2], reverse=True)
    
    for i, (true, pred, cer) in enumerate(error_data[:5]):
        print(f"{i+1}. CER={cer:.3f}")
        print(f"   –ò—Å—Ç–∏–Ω–Ω—ã–π: '{true}'")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: '{pred}'")
        print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_df = pd.DataFrame({
        'image_path': [os.path.basename(p) for p in image_paths],
        'true_text': true_texts,
        'predicted_text': predicted_texts,
        'cer': cers,
        'wer': wers,
        'exact_match': [t == p for t, p in zip(true_texts, predicted_texts)]
    })
    
    output_path = f"evaluation_results_{os.path.basename(model_path)}.csv"
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"üíæ –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ OCR –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ")
    
    parser.add_argument("--model", type=str, required=True, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ (.pth)")
    parser.add_argument("--charset", type=str, required=True, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É charset")
    parser.add_argument("--csv", type=str, required=True, help="–ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π")
    parser.add_argument("--root", type=str, required=True, help="–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏")
    parser.add_argument("--batch-size", type=int, default=16, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (default: 16)")
    parser.add_argument("--max-samples", type=int, default=None, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    parser.add_argument("--img-h", type=int, default=32, help="–í—ã—Å–æ—Ç–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (default: 32)")
    parser.add_argument("--img-w", type=int, default=128, help="–®–∏—Ä–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (default: 128)")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã
    if not os.path.exists(args.model):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.model}")
        return 1
    
    if not os.path.exists(args.charset):
        print(f"‚ùå Charset –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.charset}")
        return 1
    
    try:
        evaluate_model(
            model_path=args.model,
            charset_path=args.charset,
            csv_path=args.csv,
            root_path=args.root,
            batch_size=args.batch_size,
            max_samples=args.max_samples,
            img_h=args.img_h,
            img_w=args.img_w
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())